{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build a convolutional neural network for recognizing handwritten digits from scratch. By from scratch, I mean without using tensorflow's almighty neural network functions like `tf.nn.conv2d`. This way, you'll be able to uncover the blackbox and understand how CNN works more clearly. We'll use tensorflow interactively, so you can check the intermediate results along the way. This will also help your understanding.\n",
    "\n",
    "\n",
    "### Outline\n",
    "Here are some functions we will implement from scratch in this notebook.\n",
    "\n",
    "1. Convolutional layer\n",
    "2. ReLU\n",
    "3. Max Pooling\n",
    "4. Affine layer (Fully connected layer)\n",
    "5. Softmax\n",
    "6. Cross entropy error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines of code will download and read in the handwritten digits data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at only 100 examples at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first example of data. It's a representation of a picture as a bunch of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.38039219],\n",
       "        [ 0.37647063],\n",
       "        [ 0.3019608 ],\n",
       "        [ 0.46274513],\n",
       "        [ 0.2392157 ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.35294119],\n",
       "        [ 0.5411765 ],\n",
       "        [ 0.92156869],\n",
       "        [ 0.92156869],\n",
       "        [ 0.92156869],\n",
       "        [ 0.92156869],\n",
       "        [ 0.92156869],\n",
       "        [ 0.92156869],\n",
       "        [ 0.98431379],\n",
       "        [ 0.98431379],\n",
       "        [ 0.97254908],\n",
       "        [ 0.99607849],\n",
       "        [ 0.96078438],\n",
       "        [ 0.92156869],\n",
       "        [ 0.74509805],\n",
       "        [ 0.08235294],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.54901963],\n",
       "        [ 0.98431379],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.74117649],\n",
       "        [ 0.09019608],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.88627458],\n",
       "        [ 0.99607849],\n",
       "        [ 0.81568635],\n",
       "        [ 0.78039223],\n",
       "        [ 0.78039223],\n",
       "        [ 0.78039223],\n",
       "        [ 0.78039223],\n",
       "        [ 0.54509807],\n",
       "        [ 0.2392157 ],\n",
       "        [ 0.2392157 ],\n",
       "        [ 0.2392157 ],\n",
       "        [ 0.2392157 ],\n",
       "        [ 0.2392157 ],\n",
       "        [ 0.50196081],\n",
       "        [ 0.8705883 ],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.74117649],\n",
       "        [ 0.08235294],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.14901961],\n",
       "        [ 0.32156864],\n",
       "        [ 0.0509804 ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.13333334],\n",
       "        [ 0.83529419],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.45098042],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.32941177],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.91764712],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.32941177],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.91764712],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.41568631],\n",
       "        [ 0.6156863 ],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.95294124],\n",
       "        [ 0.20000002],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.09803922],\n",
       "        [ 0.45882356],\n",
       "        [ 0.89411771],\n",
       "        [ 0.89411771],\n",
       "        [ 0.89411771],\n",
       "        [ 0.99215692],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.94117653],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.26666668],\n",
       "        [ 0.4666667 ],\n",
       "        [ 0.86274517],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.55686277],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.14509805],\n",
       "        [ 0.73333335],\n",
       "        [ 0.99215692],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.87450987],\n",
       "        [ 0.80784321],\n",
       "        [ 0.80784321],\n",
       "        [ 0.29411766],\n",
       "        [ 0.26666668],\n",
       "        [ 0.84313732],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.45882356],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.44313729],\n",
       "        [ 0.8588236 ],\n",
       "        [ 0.99607849],\n",
       "        [ 0.94901967],\n",
       "        [ 0.89019614],\n",
       "        [ 0.45098042],\n",
       "        [ 0.34901962],\n",
       "        [ 0.12156864],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.7843138 ],\n",
       "        [ 0.99607849],\n",
       "        [ 0.9450981 ],\n",
       "        [ 0.16078432],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.66274512],\n",
       "        [ 0.99607849],\n",
       "        [ 0.6901961 ],\n",
       "        [ 0.24313727],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.18823531],\n",
       "        [ 0.90588242],\n",
       "        [ 0.99607849],\n",
       "        [ 0.91764712],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.07058824],\n",
       "        [ 0.48627454],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.32941177],\n",
       "        [ 0.99607849],\n",
       "        [ 0.99607849],\n",
       "        [ 0.65098041],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.54509807],\n",
       "        [ 0.99607849],\n",
       "        [ 0.9333334 ],\n",
       "        [ 0.22352943],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.82352948],\n",
       "        [ 0.98039222],\n",
       "        [ 0.99607849],\n",
       "        [ 0.65882355],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.94901967],\n",
       "        [ 0.99607849],\n",
       "        [ 0.93725497],\n",
       "        [ 0.22352943],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.34901962],\n",
       "        [ 0.98431379],\n",
       "        [ 0.9450981 ],\n",
       "        [ 0.33725491],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.01960784],\n",
       "        [ 0.80784321],\n",
       "        [ 0.96470594],\n",
       "        [ 0.6156863 ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.01568628],\n",
       "        [ 0.45882356],\n",
       "        [ 0.27058825],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_X, example_ys = mnist.train.next_batch(batch_size)\n",
    "example_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the convenient `InteractiveSession`, for checking the intermediate results along the way. You can now use `Tensor.eval()` and `Operation.run()` without having to specify a session explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start building the computation graph by creating placeholders for the input images(`X`) and target output labels(`t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder('float', [batch_size, 28, 28, 1])\n",
    "t = tf.placeholder('float', [batch_size, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an overview of the model we will build. It starts with a convolutional layer, pass the result to ReLU, pool, affine layer, ReLU again, second affine layer, and then apply softmax function. Keep in mind this architecture while you're following the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$$ conv - relu - pool - affine - relu - affine - softmax$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_h, filter_w, filter_c, filter_n = 5, 5, 1, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([filter_h, filter_w, filter_c, filter_n], stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros([filter_n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution(X, W, b, padding, stride):\n",
    "    n, h, w, c = map(lambda d: d.value, X.get_shape())\n",
    "    filter_h, filter_w, filter_c, filter_n = [d.value for d in W.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - filter_h)//stride + 1\n",
    "    out_w = (w + 2*padding - filter_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, filter_h, filter_w, filter_c, out_h, out_w, stride, padding)\n",
    "    W_flat = tf.reshape(W, [filter_h*filter_w*filter_c, filter_n])\n",
    "    \n",
    "    z = tf.matmul(X_flat, W_flat) + b     # b: 1 X filter_n\n",
    "    \n",
    "    return tf.transpose(tf.reshape(z, [out_h, out_w, n, filter_n]), [2, 0, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute convolution easily, we do a simple trick called flattening. After flattening, input data will be transformed into a 2D matrix, which allows for matrix multiplication with a filter (which is also flattened into 2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(X, window_h, window_w, window_c, out_h, out_w, stride=1, padding=0):\n",
    "    \n",
    "    X_padded = tf.pad(X, [[0,0], [padding, padding], [padding, padding], [0,0]])\n",
    "\n",
    "    windows = []\n",
    "    for y in range(out_h):\n",
    "        for x in range(out_w):\n",
    "            window = tf.slice(X_padded, [0, y*stride, x*stride, 0], [-1, window_h, window_w, -1])\n",
    "            windows.append(window)\n",
    "    stacked = tf.stack(windows) # shape : [out_h, out_w, n, filter_h, filter_w, c]\n",
    "\n",
    "    return tf.reshape(stacked, [-1, window_c*window_w*window_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose:0' shape=(100, 28, 28, 30) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = convolution(X, W1, b1, padding=2, stride=1)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return tf.maximum(X, tf.zeros_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Maximum:0' shape=(100, 28, 28, 30) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_activation_layer = relu(conv_layer)\n",
    "conv_activation_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(X, pool_h, pool_w, padding, stride):\n",
    "    n, h, w, c = [d.value for d in X.get_shape()]\n",
    "    \n",
    "    out_h = (h + 2*padding - pool_h)//stride + 1\n",
    "    out_w = (w + 2*padding - pool_w)//stride + 1\n",
    "\n",
    "    X_flat = flatten(X, pool_h, pool_w, c, out_h, out_w, stride, padding)\n",
    "\n",
    "    pool = tf.reduce_max(tf.reshape(X_flat, [out_h, out_w, n, pool_h*pool_w, c]), axis=3)\n",
    "    return tf.transpose(pool, [2, 0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_1:0' shape=(100, 14, 14, 30) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling_layer = max_pool(conv_activation_layer, pool_h=2, pool_w=2, padding=0, stride=2)\n",
    "pooling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, pool_output_h, pool_output_w, filter_n = [d.value for d in pooling_layer.get_shape()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of nodes in the hidden layer\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([pool_output_h*pool_output_w*filter_n, hidden_size], stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros([hidden_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine(X, W, b):\n",
    "    n = X.get_shape()[0].value # number of samples\n",
    "    X_flat = tf.reshape(X, [n, -1])\n",
    "    return tf.matmul(X_flat, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(100, 100) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_layer1 = affine(pooling_layer, W2, b2)\n",
    "affine_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.27897663e-02,   4.08502296e-03,  -7.07124034e-03,\n",
       "         2.43547782e-02,   4.93027130e-03,   9.38399695e-03,\n",
       "        -1.85905286e-04,   3.03684361e-03,   2.00360715e-02,\n",
       "        -4.39601159e-03,   4.78150760e-04,   1.28966281e-02,\n",
       "         6.84925634e-03,  -1.90684514e-03,  -8.92918836e-03,\n",
       "         2.28859372e-02,  -1.87201228e-03,   7.71142170e-03,\n",
       "         6.85415464e-03,  -6.64732605e-03,  -1.09001738e-03,\n",
       "        -1.68346390e-02,   4.27971641e-03,   6.05796045e-03,\n",
       "        -3.43236839e-03,  -1.61901501e-03,  -7.26507138e-03,\n",
       "         8.73017125e-03,  -2.07298761e-03,  -1.11386571e-02,\n",
       "        -1.18847461e-02,   4.89121629e-03,   1.33146681e-02,\n",
       "         1.06254723e-02,  -7.14600133e-03,  -4.81505925e-03,\n",
       "         3.09939869e-03,  -2.66726420e-04,  -1.16621414e-02,\n",
       "        -1.25842970e-02,   2.04771068e-02,  -8.97473190e-03,\n",
       "        -1.47461416e-02,  -4.27867286e-03,   1.62802115e-02,\n",
       "         3.26621043e-03,   3.02467291e-04,  -8.49879056e-04,\n",
       "         3.39514483e-03,   5.13623189e-03,   1.55903548e-02,\n",
       "         3.81639344e-03,  -2.42680889e-02,   2.69836653e-02,\n",
       "        -1.07115731e-02,  -1.97398756e-02,  -6.85306964e-03,\n",
       "        -4.68526362e-03,  -1.42706633e-02,   6.78107189e-03,\n",
       "        -9.57339909e-03,   2.10480336e-02,   8.77523050e-03,\n",
       "        -4.85045090e-03,  -9.78023466e-03,  -2.32692249e-02,\n",
       "        -2.07157410e-03,   5.46645559e-03,  -8.55613034e-03,\n",
       "         4.35254042e-04,  -2.88701318e-02,  -2.91350274e-03,\n",
       "        -5.04689058e-04,   1.39968488e-02,  -7.75859298e-05,\n",
       "         1.06841689e-02,  -1.09542569e-03,   1.30710378e-03,\n",
       "         9.94699541e-03,   8.81436560e-03,   1.21958926e-03,\n",
       "        -2.01405808e-02,   5.44707337e-03,  -1.12114409e-02,\n",
       "         4.90656961e-03,   3.05995881e-03,   2.18639616e-03,\n",
       "        -5.04470617e-03,   2.38656476e-02,  -1.14729768e-02,\n",
       "         2.59296177e-03,   1.38776237e-02,  -2.59603765e-02,\n",
       "         4.62312484e-03,  -6.51937618e-04,   9.01691150e-03,\n",
       "        -1.24027049e-02,   2.21672524e-02,  -5.83261775e-04,\n",
       "         6.50338177e-03], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init.run()\n",
    "affine_layer1.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows the representation of the first example as a 100 dimention vector in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Maximum_1:0' shape=(100, 100) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_activation_layer1 = relu(affine_layer1)\n",
    "affine_activation_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.00408502,  0.        ,  0.02435478,  0.00493027,\n",
       "        0.009384  ,  0.        ,  0.00303684,  0.02003607,  0.        ,\n",
       "        0.00047815,  0.01289663,  0.00684926,  0.        ,  0.        ,\n",
       "        0.02288594,  0.        ,  0.00771142,  0.00685415,  0.        ,\n",
       "        0.        ,  0.        ,  0.00427972,  0.00605796,  0.        ,\n",
       "        0.        ,  0.        ,  0.00873017,  0.        ,  0.        ,\n",
       "        0.        ,  0.00489122,  0.01331467,  0.01062547,  0.        ,\n",
       "        0.        ,  0.0030994 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.02047711,  0.        ,  0.        ,  0.        ,  0.01628021,\n",
       "        0.00326621,  0.00030247,  0.        ,  0.00339514,  0.00513623,\n",
       "        0.01559035,  0.00381639,  0.        ,  0.02698367,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00678107,\n",
       "        0.        ,  0.02104803,  0.00877523,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00546646,  0.        ,  0.00043525,\n",
       "        0.        ,  0.        ,  0.        ,  0.01399685,  0.        ,\n",
       "        0.01068417,  0.        ,  0.0013071 ,  0.009947  ,  0.00881437,\n",
       "        0.00121959,  0.        ,  0.00544707,  0.        ,  0.00490657,\n",
       "        0.00305996,  0.0021864 ,  0.        ,  0.02386565,  0.        ,\n",
       "        0.00259296,  0.01387762,  0.        ,  0.00462312,  0.        ,\n",
       "        0.00901691,  0.        ,  0.02216725,  0.        ,  0.00650338], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_activation_layer1.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is after applying ReLU to the above representation. You can see that we set all the negative numbers to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W3 = tf.Variable(tf.random_normal([hidden_size, output_size], stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros([output_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine_layer2 = affine(affine_activation_layer1, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because you have new variables, you need to initialize them.\n",
    "init = tf.global_variables_initializer()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00176254,  0.00059874,  0.00013884, -0.00036455,  0.00016459,\n",
       "        0.00127111, -0.00147798, -0.00135775,  0.00097638, -0.00073715], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_layer2.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_centered = X - tf.reduce_max(X) # to avoid overflow\n",
    "    X_exp = tf.exp(X_centered)\n",
    "    exp_sum = tf.reduce_sum(X_exp, axis=1)\n",
    "    return tf.transpose(tf.transpose(X_exp) / exp_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax_layer = softmax(affine_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09984931,  0.10008536,  0.10003935,  0.099989  ,  0.10004193,\n",
       "        0.10015268,  0.09987773,  0.09988974,  0.10012317,  0.09995175], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_layer.eval({X:example_X, t:example_ys})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got somewhat evenly distributed probabilities over 10 digits. This is as expected because we haven't trained our model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    return -tf.reduce_mean(tf.log(tf.reduce_sum(y * t, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = cross_entropy_error(softmax_layer, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025112"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.eval({X:example_X, t:example_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of times to iterate over training data\n",
    "training_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of batches\n",
    "num_batch = int(mnist.train.num_examples/batch_size)\n",
    "num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0001 cost= 0.772212091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0002 cost= 0.108307106\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    for _ in tqdm_notebook(range(num_batch)):\n",
    "        train_X, train_ys = mnist.train.next_batch(batch_size)\n",
    "        trainer.run(feed_dict={X:train_X, t:train_ys})\n",
    "        avg_cost += loss.eval(feed_dict={X:train_X, t:train_ys}) / num_batch\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = mnist.test.images[:batch_size]\n",
    "test_t = mnist.test.labels[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(network, t):\n",
    "    \n",
    "    t_predict = tf.argmax(network, axis=1)\n",
    "    t_actual = tf.argmax(t, axis=1)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98000002"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(softmax_layer, t).eval(feed_dict={X:test_x, t:test_t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 98%. Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**dreamgonfly@gmail.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
    "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
    "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
    "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
    "\n",
    "  ga('create', 'UA-91026007-1', 'auto');\n",
    "  ga('send', 'pageview');\n",
    "\n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {
    "065d38bfc86543d6bc25b291f29108e4": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "0b05e2f081a449d189273f1ed62c90c1": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "d75a7be8f9694d00a4f2836cb6d2b016": {
     "views": [
      {
       "cell_index": 53
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
